name: Pegos Twitter Scraper (JSON → HF)

on:
  workflow_dispatch:
  schedule:
    - cron: "0 */4 * * *"   # 4 saatte bir (UTC)

jobs:
  run-scraper:
    runs-on: ubuntu-latest
    timeout-minutes: 40

    env:
      AUTH_TOKEN: ${{ secrets.AUTH_TOKEN }}
      CT0: ${{ secrets.CT0 }}
      HF_TOKEN: ${{ secrets.HF_TOKEN }}
      HF_REPO_ID: ${{ secrets.HF_REPO_ID }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Run scraper
        run: |
          python scraper.py

      - name: Upload data folder to Hugging Face dataset (MANDATORY)
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          HF_REPO_ID: ${{ secrets.HF_REPO_ID }}
        run: |
          python - << 'PY'
          import os
          from huggingface_hub import HfApi

          token = os.environ.get("HF_TOKEN")
          repo_id = os.environ.get("HF_REPO_ID")

          if not token or not repo_id:
            raise SystemExit("❌ HF_TOKEN veya HF_REPO_ID yok. Secrets tanımlarını kontrol et.")

          api = HfApi()
          api.upload_folder(
              folder_path="data",
              repo_id=repo_id,
              repo_type="dataset",
              token=token,
              commit_message="GitHub Actions scrape update"
          )
          print("✅ HF UPLOAD DONE")
          PY
